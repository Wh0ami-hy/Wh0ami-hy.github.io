---
layout: post   	
catalog: true 	
tags:
    - python
---



## 简介

**Requests**库是基于python的亲儿子**urllib**模块进一步开发的**HTTP请求库**，与urllib相比使用更简单，不再需要自己构建各种Request、opener和handler，只需要在请求时，传入相应的参数即可，比之更优雅。



## 知识体系

- requests库是HTTP请求库
- 获取的数据一般存在两种格式：HTML格式、JSON格式

### 掌握Requests库

* 必须要了解**HTTP协议**内容；

* 获取想要的数据内容，就必须了解**Json/HTML/JavaScript/CSS**等前端基础知识，还有**正则表达式**/**Xpath**/**bs4**数据解析方法；

* 有些网站或者接口需要登录，才能获取数据，还需要学习一些**加密/解密/编码/鉴权机制**的知识。

## Requests库的8个主要方法

| 方法               | 描述                                                         |
| ------------------ | ------------------------------------------------------------ |
| requests.request() | 构造一个请求，是支撑以下各方法的基础方法                     |
| requests.get()     | 获取HTML网页的主要方法，对应于HTTP的Get                      |
| requests.head()    | 获取HTML网页头信息的方法，对应于HTTP的Head                   |
| requests.post()    | 向HTML网页提交Post请求的方法，对应于Http的Post               |
| requests.put()     | 向HTML网页提交Put请求的方法，对应于HTTP的put                 |
| requests.patch()   | 向HTML网页提交局部修改请求，对应于HTTP的Patch                |
| requests.delete()  | 向HTML页面提交删除请求，对应于HTTP的Delete                   |
| requests.options() | 向HTML页面预提交一个请求，可以得知服务器支持的请求方式，对应于HTTP的Options |

## 请求中的参数

### 请求时传参

* GET传参

可以直接**在URL中输入参数**，或者使用参数**params**传参。

```python
resp1 = requests.get("http://httpbin.org/get?key1=value1")
# 或者
resp2 = requests.get("http://httpbin.org/get", params={"key2":"value2"}) 
```

* POST传参

通过参数**data**，向URL提交一个字典形式的表单，通常在注册/登录的时候使用

```python
data = {
"username": "admin",
"password": "123456"
}
resp = requests.post('http://httpbin.org/post',data=data)
```

发送**json**数据

```python
json = {
"username": "admin",
"password": "123456"
}
resp = requests.post('https://httpbin.org/post', json=json)
```

* 在选择data还是json传参时，一般取决于请求头中的`Content-Type`

### POST上传文件

使用**files**参数，如传一张图片

```python
fp = {'file': open('1.png', 'rb')}
# 使用files参数就可以了
resp = requests.request('POST','http://httpbin.org/post',files=fp)
```

| 参数                 |
| -------------------- |
| method               |
| url                  |
| params=None          |
| data=None            |
| headers=None         |
| cookies=None         |
| files=None           |
| auth=None            |
| timeout=None         |
| allow_redirects=True |
| proxies=None         |
| hooks=None           |
| stream=None          |
| verify=None          |
| cert=None            |
| json=None            |

### method

请求的方法

### url

传入目标网站地址

```
resp = requests.get(url="http://httpbin.org/get")# 
或者resp = requests.request('get', url="http://httpbin.org/get")
```

### params

字典或字节序列，通常作为GET请求的参数增加到url中

### data

字典/字节序列，作为POST请求的参数传入请求体中 

### json

传入JSON格式的数据，作为请求体中的内容

### headers

字典形式, 用于HTTP中自定义请求头，使爬虫更好地伪装成浏览器客户端，起到一定的反反爬作用

### cookies

字典或**RequestsCookieJar**类型，在请求中添加cookies用以访问一些需要登录验证的网站，也可以起到一定的反反爬作用（注意：cookies是cookie的复数形式，没有特殊的区别，在python中指的都是一个意思，）**
1.获取cookies的方式**

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)

使用响应对象Response的cookies属性方法，可以直接从响应中获取到cookie，获取到的对象cookie_1是**RequestsCookieJar**类型的，适合跨域名跨路径使用。可以将其转换成字典的形式cookie_2。还可以使用**items()**方法将RequestsCookieJar类型，转换成一个元组列表的形式，通过遍历列表，可以把每个元组取出来。

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)

扩展

```
# 将CookieJar转为字典：cookie_dict = requests.utils.dict_from_cookiejar(resp.cookies)print(cookie_dict)# 将字典转为CookieJar：cookies = requests.utils.cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True)print(cookies)
```


2.使用cookies
**(注意：字典或RequestsCookieJar类型的都可以！cookie_1和cookie_2)
2.1构造cookie参数传入获取到的cookies

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)



2.2放入请求头中

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)

 

### auth

传入元组，在python中支持HTTP指定身份验证认证功能。当网站中只有指定请求头的验证方式，才能访问的时候，可以通过配置这个参数指定身份验证。这属于高级用法了，解释起来比较复杂，大概了解一下。

Requests在requests.auth中提供了两种常见的的身份验证方案HTTPBasicAuth和HTTPDigestAuth。

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)

### files

字典类型的文件，用于传输图片/文件等。上面介绍POST请求时举过实例了。

### timeout

设定请求超时的时间，单位为秒，一个请求未响应的时间超过预设置的时间，返回请求超时。 
扩展：

> 在多数爬虫开发中，超时参数timeout和retrying(刷新)模块一起使用。
>
> 1. 使用retrying模块提供的retry方法
> 2. 通过装饰器的方式，让被装饰的函数反复执行
> 3. retry中可以传入参数 stop_max_attempt_number，让函数报错后继续重新执行，达到最大执行次数的上限，如果每次都报错，整个函数报错，如果中间有一个成功，程序继续往后执行
>
> https://blog.csdn.net/qq_44907926/article/details/118667559

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)

简单的说明：当请求一个网站，会因为网络或其他原因，一时半会儿获取不到响应，脚本就会卡死或者报错。设置一个超时时间，当达到预设时间还是没有响应，使用刷新**装饰器**，刷新几次还是不行的话，再报错。有一次成功了，那就继续。


### proxies

字典类型，给脚本设置**代理IP**，一般用于反反爬中，让目标服务以为脚本的请求不是来自同一个主机，防止自己的IP因为频繁请求而被目标服务器拉黑封禁，IP被关进小黑屋，就不能再访问该网站啦。

代理IP(这里指正向代理)，也叫代理服务器，就是把本机的请求，转发给一个代理(中介)，然后这个代理，把请求头信息**修改**或**不修改**，转发给目标网站服务器。获取到的响应，也会经过代理服务器，返回到本机。

代理的分类：一般分为**HTTP代理**、**HTTPS代理**、**Socks代理**。结合网站的协议使用HTTP或者HTTPS代理，一般这两个一起使用。而Socks代理是全能代理。支持HTTP、FTP及其他类型的请求。Socks代理又分为**Socks4**和**Socks5**。Socks4仅支持TCP协议的请求。Socks5支持TCP和UDP协议的请求，还支持身份验证机制的协议请求，标准端口号为**1080**。上个图直观一点：

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)


扩展：
代理IP从隐秘性上划分，可以分为**透明代理**、**匿名代理**、**高匿名代理**。其本质就是代理服务器是否对脚本的请求头信息进行加工处理。

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)

吐槽：网上很多免费的代理IP，但是很多都是不能用的，之前写过脚本去获取并测试，一万个里能用的不到半百......买又买不起！穷~


使用代理（单复数形式都可）

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)

### allow_redirects

True/False,重定向开关,默认为True。遇到重定向的网站会自动跟踪访问。

### stream

True/False，获取内容是否立即下载，默认为True

### verity

True/False，是否开启ssl证书验证，默认Ture。

发起请求的时候，requests会对网站进行**安全证书验证**，以判断识别网站是存在风险的还是安全的。相信平时在访问某些不可描述的网站的时候，都遇到过，尤其是使用火狐浏览器访问。在浏览器，可以通过点击接受风险、忽略警告继续访问。而在requests中也可以通过**verity**参数设置，跳过验证，再使用其他的函数忽略警告。

![图片](F:%5C%E7%AC%94%E8%AE%B0%5C%E5%8D%9A%E5%AE%A2%5C%E6%96%87%E7%AB%A0%E5%9B%BE%E7%89%87%5C640)

(有点翻车，无证书访问没有抛异常【ssl.SSLError】，只是给出了警告，估计是专业版的Pycharm给优化了吧，或者其他原因。)

上面的代码，关闭所有的警告，个人感觉并不是很好，因为有时候是其他原因造成的警告，需要查看排查原因。建议使用下面的代码，仅关闭安全提示的警告。



```
from requests.packages.urllib3.exceptions import InsecureRequestWarningrequests.packages.urllin3.disablewarnings(InsecureRequestWarning)
```



###  cert

传入一个ssl证书，指定ssl证书路径

 有时我们需要设置代理脚本或者代理抓包工具，而不是跳过安全验证。就可以在本地上传一个证书，对网站进行安全验证。通过cert参数设置证书。证书包含一个证书文件和一个密钥。

```
resp = requests.get('https://www.12306.cn', cert=('本地证书名称的路径', '证书密钥路径'))
```

### hooks

即钩子方法，属于高阶用法。可以在请求获取到响应之后，去执行一些自定义的操作，比如获取响应信息、打印信息、修改响应内容等。

 **小总结**

在爬虫和接口测试学习中，初级只需要了解GET和POST请求方法，及headers、cookies、proxies、params、data、json就够用了。其他高阶用法，遇到问题的时候，再去研究。

## 响应Response

```python
import requests
resp = requests.get（'http://httpbin.org/get')
```

resp——**响应对象**，包含从服务中返回的响应数据内容

### 响应对象的属性

| 属性                   | 说明                                                         |
| ---------------------- | ------------------------------------------------------------ |
| resp.status_code       | 输出HTTP请求的响应状态码，200表示连接成功，404表示失败       |
| resp.text              | 输出HTTP响应的Unicode字符形式的文本内容，就跟浏览器打开F12看到的文本内容是一毛一样的 |
| resp.encoding          | 从HTTP头部中猜测的响应内容编码方式                           |
| resp.apparent_encoding | 自动从响应中分析出的内容的编码方式，并设置相应的编码方式     |
| resp.content           | HTTP响应内容的二进制形式，比如在下载图片/视频/音频文件的时候，就需要把数据转换成二进制的数据流进行传输 |
| resp.cookies           | 输出从响应内容中获取cookies信息                              |
| resp.url               | 获取响应中的地址                                             |
| resp.close             | 关闭响应对象                                                 |

### 自动识别编码

有时候网站返回的数据是各种乱七八糟看不懂的符号，就需要对内容进行编码操作

扩展编码方式：

在requests的响应对象Response中，默认编码方式是**ISO-8859-1**，而一般的网站内容编码方式为**utf-8**或者**GBK**，resp.text会根据resp.encoding的编码方式显示内容。

```python
import requests

resp = requests.get(url='https://www.baidu.com')
resp.encoding = resp.apparent_encoding          # 自动识别网站编码
print(resp.text)   
```



##  其他扩展

### session

在request中提供了requests.Session()对象，可以实现**跨请求**保留某些参数。官方解释：

Provides cookie persistence, connection-pooling, and configuration.

举个例子，比如使用session成功的登录了目标网站，登录信息中的cookies等参数就会保存在session对象中，在对目标网站内其他页面操作时，session就会携带cookies等参数信息，发起请求。就不会出现访问站内的其他地址，又需要重新登录，或者手动添加cookies等参数的情况。

## 爬虫通用框架

```python
import requests


def get_html(url, **keyword):
    try:
        response = requests.get(url)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return response.text
    except:
        return "响应错误啦！是不是代码又写错了？！"


if __name__ == "__main__":
    url = "https://www.baidu.com"
    print(get_html(url))
```

